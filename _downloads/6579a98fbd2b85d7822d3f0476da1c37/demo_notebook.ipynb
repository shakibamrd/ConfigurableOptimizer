{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Configurable Optimizers ‚ö°\n",
    "\n",
    "We designed this library to abstract orthogonal components in the Differentiable NAS Research. But what are these orthogonal components?\n",
    "\n",
    "Consider, for instance, the [PC-DARTS](https://arxiv.org/abs/1907.05737) method. It has 3 components if we see from high level- a **searchspace** (DARTS), a **sampler** (softmax) to continously sample architectures, and a feature, **partial-connections**. Let use call these one-shot components that we could *plug* üîå into searchspace and *play*! üéÆ\n",
    "\n",
    "There are many such components from the past researches in the Differentiable NAS Research. We introduce a high-level API, i.e., **Profile** ü•∑ - through which a user can ***configure*** everything about their method.\n",
    "\n",
    "See this simple example of how we can connect these components together to run PC-DARTS method.\n",
    "\n",
    "*Before proceeding, please follow the installation instructions in the README.md to install the library to your preffered environment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confopt.profile import DARTSProfile\n",
    "from confopt.train import Experiment\n",
    "from confopt.enums import SearchSpaceType, DatasetType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Usage ‚öíÔ∏è\n",
    "\n",
    "We have some pre-configured profiles like \n",
    "- ***DARTSProfile*** for softmax sampling,\n",
    "- ***DrNASProfile*** for sampling from a dirchlet distribution,\n",
    "- ***GDASProfile*** for sampling from a gumbel-softmax distribution.\n",
    "\n",
    "There are plenty of options available in these profiles, but for now, we will stick to a minimal version...\n",
    "\n",
    "Since we are gonna do a softmax sampling, we are gonna use the **DARTSProfile**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = DARTSProfile(\n",
    "    searchspace_type=SearchSpaceType.DARTS,\n",
    "    epochs=3,\n",
    "    seed=100,\n",
    "    is_partial_connection=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our profile, we want to setup another core fragment of the library - ***Experiment*** üß™\n",
    "\n",
    "The ***Experiment*** class defines essentially what the profile is applied on. It takes in `search-space`, `dataset`, `seed`. For the purpose of demo, we will also use an option called `debug_mode` to run it only for testing purpose.    \n",
    "\n",
    "***Experiment*** in turn, provides api to train the supernetwork and the discrete networks via *train_supernet* and *train_discrete_model* functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "    search_space=SearchSpaceType.DARTS,\n",
    "    dataset=DatasetType.CIFAR10,\n",
    "    seed=100,\n",
    "    debug_mode=True,\n",
    "    exp_name=\"simple-example\",\n",
    ")\n",
    "\n",
    "experiment.train_supernet(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Usage ‚öíÔ∏è\n",
    "\n",
    "Having been familiar with our workflow üìÉ, we can move to explore some more interesting options we provide in the Profile. \n",
    "\n",
    "From plethora of components, let us make a custom profile which will use **GDAS** sampler, that would have [random perturbation](https://arxiv.org/abs/2002.05283), would use [LoRA layers](https://openreview.net/forum?id=YNyTumD0U9&referrer=%5Bthe%20profile%20of%20Frank%20Hutter%5D(%2Fprofile%3Fid%3D~Frank_Hutter1)) with the operations. To top it off, let us also use the [operation-level early stopping](https://proceedings.neurips.cc/paper_files/paper/2023/file/e0bc6dbcbcc957b2aeadb20c39ba7f05-Paper-Conference.pdf), which freezes the operations when they start to overfit.\n",
    "\n",
    "In the interest to show variety of searchspace we have, we will use **NB201SearchSpace** for this next example. \n",
    "\n",
    "\n",
    "*Note:*\n",
    "- For all options, checkout [BaseProfile](https://github.com/automl/ConfigurableOptimizer/blob/main/src/confopt/profile/base.py).\n",
    "    \n",
    "- We support currently 6 searchspaces, [DARTS](https://arxiv.org/abs/1806.09055), [NB201](https://arxiv.org/abs/2001.00326), [NB1SHOT1](https://arxiv.org/abs/2001.10422), [TNB101](https://arxiv.org/abs/2105.11871), [RobustDARTS](https://arxiv.org/abs/1909.09656) and BABYDARTS.\n",
    "    - Here, the BABYDARTS search space is designed as a toy searchspace for tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confopt.profile import GDASProfile\n",
    "\n",
    "profile = GDASProfile(\n",
    "    searchspace_type=SearchSpaceType.NB201,\n",
    "    epochs=10,\n",
    "    perturbation=\"random\",\n",
    "    lora_rank=1,\n",
    "    lora_warm_epochs=3,\n",
    "    oles=True,\n",
    "    calc_gm_score=True,\n",
    "    seed=100,\n",
    ")\n",
    "\n",
    "profile.configure_oles(frequency=30, threshold=0.4)\n",
    "profile.configure_lora(\n",
    "    r=2, lora_alpha=1, lora_dropout=0.1\n",
    ")  # overwrite previous rank!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw above, you could also configure the methods after initializing the Profile. \n",
    "- After `oles` is provided with the Profile, you can configure oles related arguments like *frequency* of steps to measure gm scores, *threshold* to use for early-stop an operation etc.\n",
    "- Similarly, after lora layers are enabled from Profile, you can configure rank to be lora-related configs, like the *lora alpha*, and *lora dropout* probability for lora layers.\n",
    "\n",
    "Let us also configure training configs as well, with a batch-size of 96, and use a learning rate of 0.04 for training the supernet and 3e-4 for the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.configure_trainer(lr=0.04, arch_lr=3e-4, batch_size=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the experiments that we run are logged in a local `log` ü™µ folder, where we save genotypes, model checkpoints, and std logs. These are very helpful to look at after you have finished an experiment run. Additionally, for a better management, we also have an option to log stuff on [**WandB**](https://wandb.ai/) ü™Ñ. \n",
    "\n",
    "We also track a lot of metrics which can be helpful to analyse üî¨ experiment like -\n",
    "- Frequency of operation being picked in genotype per epoch.\n",
    "- Gradient norms of cells and edges.\n",
    "- Gradient matching scores for operations.\n",
    "- alpha values for edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many custom tags/configs here to differentiate runs on WandB\n",
    "profile.configure_extra(\n",
    "    project_name=\"advanced-example\",  # Name of the Wandb Project\n",
    "    run_purpose=\"test\",  # Purpose of the run\n",
    ")\n",
    "\n",
    "experiment = Experiment(\n",
    "    search_space=SearchSpaceType.NB201,\n",
    "    dataset=DatasetType.CIFAR10,\n",
    "    seed=100,\n",
    "    debug_mode=True,\n",
    "    exp_name=\"advanced-example\",\n",
    "    log_with_wandb=True,  # enable logging with Weights and Biases\n",
    ")\n",
    "\n",
    "experiment.train_supernet(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Discrete Model üöÖ\n",
    "\n",
    "You have searched for the model. You got an architecture, but how do you test that this architecture is even good ü§î? *We got you covered!*\n",
    "\n",
    "We have the **DiscreteProfile** that lets you train your model. Every searchspace has their own genotype structure. You should be able to check logs folder to find the best genotype found through the search (we did earlier).\n",
    "\n",
    "For this example, we would pick the best genotype found by the vanilla DARTS method. Lets take a look at how the genotype looks like -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from io import BytesIO\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "from confopt.searchspace import DARTSGenotype  # noqa: F401\n",
    "\n",
    "\n",
    "def plot(genotype, genotype_title):\n",
    "    g = Digraph(\n",
    "        format=\"pdf\",\n",
    "        edge_attr=dict(fontsize=\"20\", fontname=\"times\"),\n",
    "        node_attr=dict(\n",
    "            style=\"filled\",\n",
    "            shape=\"rect\",\n",
    "            align=\"center\",\n",
    "            fontsize=\"20\",\n",
    "            height=\"0.5\",\n",
    "            width=\"0.5\",\n",
    "            penwidth=\"2\",\n",
    "            fontname=\"times\",\n",
    "        ),\n",
    "        engine=\"dot\",\n",
    "    )\n",
    "    g.attr(dpi=\"600\")\n",
    "    g.body.extend([\"rankdir=LR\"])\n",
    "\n",
    "    g.node(\"c_{k-2}\", fillcolor=\"darkseagreen2\")\n",
    "    g.node(\"c_{k-1}\", fillcolor=\"darkseagreen2\")\n",
    "    assert len(genotype) % 2 == 0\n",
    "    steps = len(genotype) // 2\n",
    "\n",
    "    for i in range(steps):\n",
    "        g.node(str(i), fillcolor=\"lightblue\")\n",
    "\n",
    "    for i in range(steps):\n",
    "        for k in [2 * i, 2 * i + 1]:\n",
    "            op, j = genotype[k]\n",
    "            if j == 0:\n",
    "                u = \"c_{k-2}\"\n",
    "            elif j == 1:\n",
    "                u = \"c_{k-1}\"\n",
    "            else:\n",
    "                u = str(j - 2)\n",
    "            v = str(i)\n",
    "            g.edge(u, v, label=op, fillcolor=\"gray\")\n",
    "\n",
    "    g.node(\"c_{k}\", fillcolor=\"palegoldenrod\")\n",
    "    for i in range(steps):\n",
    "        g.edge(str(i), \"c_{k}\", fillcolor=\"gray\")\n",
    "\n",
    "    img_bytes = g.pipe(format=\"png\")  # Render as PNG bytes\n",
    "    img = PIL.Image.open(BytesIO(img_bytes))  # Open with PIL\n",
    "\n",
    "    # Display in Matplotlib\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")  # Hide axes\n",
    "    plt.title(genotype_title, fontsize=12, pad=10)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_genotype(genotype_str):\n",
    "    try:\n",
    "        genotype = eval(genotype_str)\n",
    "    except AttributeError:\n",
    "        print(\"{} is not specified in genotypes.py\".format(genotype_str))\n",
    "\n",
    "    plot(genotype.normal, \"normal cell\")\n",
    "    plot(genotype.reduce, \"reduction cell\")\n",
    "\n",
    "\n",
    "genotype_str = str(\n",
    "    DARTSGenotype(\n",
    "        normal=[\n",
    "            (\"sep_conv_3x3\", 0),\n",
    "            (\"sep_conv_3x3\", 1),\n",
    "            (\"sep_conv_3x3\", 0),\n",
    "            (\"sep_conv_3x3\", 1),\n",
    "            (\"sep_conv_3x3\", 1),\n",
    "            (\"skip_connect\", 0),\n",
    "            (\"skip_connect\", 0),\n",
    "            (\"dil_conv_3x3\", 2),\n",
    "        ],\n",
    "        normal_concat=[2, 3, 4, 5],\n",
    "        reduce=[\n",
    "            (\"max_pool_3x3\", 0),\n",
    "            (\"max_pool_3x3\", 1),\n",
    "            (\"skip_connect\", 2),\n",
    "            (\"max_pool_3x3\", 1),\n",
    "            (\"max_pool_3x3\", 0),\n",
    "            (\"skip_connect\", 2),\n",
    "            (\"skip_connect\", 2),\n",
    "            (\"max_pool_3x3\", 1),\n",
    "        ],\n",
    "        reduce_concat=[2, 3, 4, 5],\n",
    "    )\n",
    ")\n",
    "plot_genotype(genotype_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`DiscreteProfile`** has an attribute `genotype` that we would set to train the above architecture. With **`DiscreteProfile`**, we can also directly set the trainer config within the initializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confopt.profile import DiscreteProfile\n",
    "\n",
    "discrete_profile = DiscreteProfile(\n",
    "    searchspace_type=SearchSpaceType.DARTS,\n",
    "    epochs=10,\n",
    "    seed=100,\n",
    "    batch_size=96,\n",
    "    lr=0.03,\n",
    ")\n",
    "\n",
    "discrete_profile.configure_extra(\n",
    "    project_name=\"Train-Discrete-Model\",\n",
    "    tag=\"discrete-run\",\n",
    ")\n",
    "discrete_profile.genotype = genotype_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined a profile for training architecture, we can now go on to intialize the **`Experiment`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "    search_space=SearchSpaceType.DARTS,\n",
    "    dataset=DatasetType.CIFAR10,\n",
    "    seed=100,\n",
    "    log_with_wandb=True,\n",
    "    debug_mode=True,\n",
    "    exp_name=\"discrete-demo\",\n",
    ")\n",
    "\n",
    "print(\"Training model with genotype: \", discrete_profile.genotype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expose another api for training discrete model called *`train_discrete_model`* for training a model based on the configurations defined in the `DiscreteProfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.train_discrete_model(discrete_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope you had fun with this tutorial! üëã\n",
    "\n",
    "üîÑ We'd be updating our docs soon with more advanced examples. **Stay tuned!** üîÑ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "confopt-home",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
