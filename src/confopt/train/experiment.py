from __future__ import annotations

import argparse
from collections import namedtuple
import json
import random
from typing import Any, Callable, Literal
import warnings

import numpy as np
import torch
from torch.backends import cudnn
import wandb

from confopt.dataset import get_dataset
from confopt.dataset.data import AbstractData
from confopt.enums import (
    CriterionType,
    DatasetType,
    OptimizerType,
    PerturbatorType,
    SamplerType,
    SchedulerType,
    SearchSpaceType,
)
from confopt.oneshot import (
    DrNASRegularizationTerm,
    Dropout,
    EarlyStopper,
    FairDARTSRegularizationTerm,
    FLOPSRegularizationTerm,
    LoRAToggler,
    PartialConnector,
    Pruner,
    RegularizationTerm,
    Regularizer,
    SDARTSPerturbator,
    SkipConnectionEarlyStopper,
    WeightEntangler,
)
from confopt.oneshot.archsampler import (
    BaseSampler,
    DARTSSampler,
    DRNASSampler,
    GDASSampler,
    ReinMaxSampler,
    SNASSampler,
)
from confopt.profile import (
    BaseProfile,
    DiscreteProfile,
    GDASProfile,
)
from confopt.searchspace import (
    BabyDARTSSearchSpace,
    DARTSGenotype,  # noqa: F401
    DARTSImageNetModel,
    DARTSModel,
    DARTSSearchSpace,
    NAS201Genotype,
    NASBench1Shot1SearchSpace,
    NASBench201Model,
    NASBench201SearchSpace,
    RobustDARTSSearchSpace,
    SearchSpace,
    TransNASBench101SearchSpace,
)
from confopt.searchspace.common import LambdaReg
from confopt.train import ConfigurableTrainer, DiscreteTrainer
from confopt.train.projection import PerturbationArchSelection
from confopt.train.search_space_handler import SearchSpaceHandler
from confopt.utils import Logger, validate_model_to_load_value
from confopt.utils import distributed as dist_utils
from confopt.utils.time import check_date_format

DEVICE = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")


class Experiment:
    def __init__(
        self,
        search_space: SearchSpaceType,
        dataset: DatasetType,
        seed: int,
        log_with_wandb: bool = False,
        debug_mode: bool = False,
        exp_name: str = "test",
        dataset_domain: str | None = None,
        dataset_dir: str = "datasets",
        api_dir: str = "api",
    ) -> None:
        self.searchspace_type = search_space
        self.dataset = dataset
        self.dataset_domain = dataset_domain
        self.seed = seed
        self.log_with_wandb = log_with_wandb
        self.debug_mode = debug_mode
        self.exp_name = exp_name
        self.dataset_dir = dataset_dir
        self.api_dir = api_dir

    def _set_seed(self, rand_seed: int) -> None:
        random.seed(rand_seed)
        np.random.seed(rand_seed)
        cudnn.benchmark = True
        torch.manual_seed(rand_seed)
        cudnn.enabled = True
        torch.cuda.manual_seed(rand_seed)

    def init_ddp(self) -> None:
        dist_utils.init_distributed()

    def cleanup_ddp(self) -> None:
        dist_utils.cleanup()

    def train_supernet(
        self,
        profile: BaseProfile,
        model_to_load: str | int | None = None,
        exp_runtime_to_load: str | None = None,
        use_benchmark: bool = False,
    ) -> ConfigurableTrainer:
        config = profile.get_config()
        run_name = profile.get_run_description()
        config["dataset"] = self.dataset.value

        assert hasattr(profile, "sampler_type")
        self.sampler_str = SamplerType(profile.sampler_type)
        self.perturbator_str = PerturbatorType(profile.perturb_type)
        self.is_partial_connection = profile.is_partial_connection
        self.dropout_p = profile.dropout
        self.edge_normalization = profile.is_partial_connection
        self.entangle_op_weights = profile.entangle_op_weights
        oles_config = config["oles"]

        return self._train_supernet(
            config=config,
            model_to_load=model_to_load,
            exp_runtime_to_load=exp_runtime_to_load,
            use_benchmark=use_benchmark,
            run_name=run_name,
            calc_gm_score=oles_config["calc_gm_score"],
            oles=oles_config["oles"],
            oles_frequency=oles_config["frequency"],
            oles_threshold=oles_config["threshold"],
        )

    def _init_wandb(self, run_name: str, config: dict) -> None:
        wandb.init(  # type: ignore
            name=run_name,
            project=(
                config.get("project_name", "Configurable_Optimizer")
                if config is not None
                else "Configurable_Optimizer"
            ),
            config=config,
        )

    def _train_supernet(
        self,
        config: dict | None = None,
        model_to_load: str | int | None = None,
        exp_runtime_to_load: str | None = None,
        use_benchmark: bool = False,
        run_name: str = "supernet_run",
        calc_gm_score: bool = False,
        oles: bool = False,
        oles_frequency: int = 20,
        oles_threshold: float = 0.4,
    ) -> ConfigurableTrainer:
        self._set_seed(self.seed)

        load_last_run = model_to_load is not None and not exp_runtime_to_load

        self.logger = Logger(
            log_dir="logs",
            exp_name=self.exp_name,
            search_space=self.searchspace_type.value,
            dataset=str(self.dataset.value),
            seed=self.seed,
            runtime=exp_runtime_to_load,
            use_supernet_checkpoint=True,
            last_run=load_last_run,
        )

        self.logger.log(
            "Logs and checkpoints will be saved in the following directory: "
            + self.logger.path(None)
        )
        config["save_dir"] = self.logger.path(None)  # type:ignore

        self._init_components(
            self.searchspace_type,
            self.sampler_str,
            self.perturbator_str,
            config=config,
            use_benchmark=use_benchmark,
        )
        if self.log_with_wandb:
            self._init_wandb(run_name, config)  # type: ignore

        trainer = self._initialize_configurable_trainer(
            config=config,  # type: ignore
            model_to_load=model_to_load,
        )

        config_str = json.dumps(config, indent=2, default=str)
        self.logger.log(
            f"Training the supernet with the following configuration: \n{config_str}"
        )

        trainer.train(
            search_space_handler=self.search_space_handler,  # type: ignore
            log_with_wandb=self.log_with_wandb,
            lora_warm_epochs=config["trainer"].get(  # type: ignore
                "lora_warm_epochs", 0
            ),
            calc_gm_score=calc_gm_score,
            oles=oles,
            oles_frequency=oles_frequency,
            oles_threshold=oles_threshold,
        )

        return trainer

    def _init_components(
        self,
        searchspace_type: SearchSpaceType,
        sampler_type: SamplerType,
        perturbator_type: PerturbatorType,
        config: dict | None = None,
        use_benchmark: bool = False,
    ) -> None:
        if config is None:
            config = {}  # type : ignore
        self._set_search_space(searchspace_type, config.get("search_space", {}))
        self._set_sampler(sampler_type, config.get("sampler", {}))
        self._set_perturbator(perturbator_type, config.get("perturbator", {}))
        self._set_partial_connector(config.get("partial_connector", {}))
        self._set_dropout(config.get("dropout", {}))
        self._set_pruner(config.get("pruner", {}))
        self.benchmark_api: None | Any = None

        if use_benchmark:
            if (
                searchspace_type == SearchSpaceType.RobustDARTS
                and config.get("search_space", {}).get("space") == "s4"
            ):
                warnings.warn(
                    "Argument use_benchmark was set to True with s4 space of"
                    + " RobustDARTSSearchSpace. Consider setting it to False",
                    stacklevel=1,
                )
                self.benchmark_api = None
            else:
                self._set_benchmark_api(searchspace_type, config.get("benchmark", {}))
        else:
            self.benchmark_api = None

        self._set_lora_toggler(config.get("lora", {}), config.get("lora_extra", {}))
        self._set_weight_entangler()
        self._set_regularizer(config.get("regularization", {}))
        self._set_lambda_regularizer(config.get("lambda_regularizer", {}))
        self._set_profile(config)
        self._set_early_stopper(
            config["early_stopper"], config.get("early_stopper_config", {})
        )

    def _set_search_space(
        self,
        search_space: SearchSpaceType,
        config: dict,
    ) -> None:
        if search_space == SearchSpaceType.NB201:
            self.search_space = NASBench201SearchSpace(**config)
        elif search_space == SearchSpaceType.DARTS:
            self.search_space = DARTSSearchSpace(**config)
        elif search_space == SearchSpaceType.NB1SHOT1:
            self.search_space = NASBench1Shot1SearchSpace(**config)
        elif search_space == SearchSpaceType.TNB101:
            self.search_space = TransNASBench101SearchSpace(**config)
        elif search_space == SearchSpaceType.BABYDARTS:
            self.search_space = BabyDARTSSearchSpace(**config)
        elif search_space == SearchSpaceType.RobustDARTS:
            self.search_space = RobustDARTSSearchSpace(**config)

    def _set_benchmark_api(
        self,
        search_space: SearchSpaceType,
        config: dict,
    ) -> None:
        if search_space == SearchSpaceType.NB1SHOT1:
            from confopt.benchmark import NB101Benchmark

            self.benchmark_api = NB101Benchmark("full", self.api_dir)
        elif search_space == SearchSpaceType.NB201:
            from confopt.benchmark import NB201Benchmark

            self.benchmark_api = NB201Benchmark(self.api_dir)
        elif search_space in (SearchSpaceType.DARTS, SearchSpaceType.RobustDARTS):
            from confopt.benchmark import NB301Benchmark

            self.benchmark_api = NB301Benchmark(api_root_dir=self.api_dir, **config)
        elif search_space == SearchSpaceType.TNB101:
            from confopt.benchmark import TNB101Benchmark

            self.benchmark_api = TNB101Benchmark(self.api_dir)
        else:
            print(f"Benchmark does not exist for the {search_space.value} searchspace")
            self.benchmark_api = None

    def _set_sampler(
        self,
        sampler: SamplerType,
        config: dict,
    ) -> None:
        arch_params = self.search_space.arch_parameters
        self.sampler: BaseSampler | None = None
        if sampler in (SamplerType.DARTS, SamplerType.LAMBDADARTS):
            self.sampler = DARTSSampler(**config, arch_parameters=arch_params)
        elif sampler == SamplerType.DRNAS:
            self.sampler = DRNASSampler(**config, arch_parameters=arch_params)
        elif sampler == SamplerType.GDAS:
            self.sampler = GDASSampler(**config, arch_parameters=arch_params)
        elif sampler == SamplerType.SNAS:
            self.sampler = SNASSampler(**config, arch_parameters=arch_params)
        elif sampler == SamplerType.REINMAX:
            self.sampler = ReinMaxSampler(**config, arch_parameters=arch_params)

    def _set_perturbator(
        self,
        petubrator_type: PerturbatorType,
        pertub_config: dict,
    ) -> None:
        self.perturbator: SDARTSPerturbator | None = None
        if petubrator_type != PerturbatorType.NONE:
            self.perturbator = SDARTSPerturbator(
                **pertub_config,
                search_space=self.search_space,
                arch_parameters=self.search_space.arch_parameters,
                attack_type=petubrator_type.value,  # type: ignore
            )

    def _set_partial_connector(self, config: dict) -> None:
        self.partial_connector: PartialConnector | None = None
        if self.is_partial_connection:
            self.partial_connector = PartialConnector(**config)

    def _set_dropout(self, config: dict) -> None:
        self.dropout: Dropout | None = None
        if self.dropout_p is not None:
            self.dropout = Dropout(**config)

    def _set_weight_entangler(self) -> None:
        self.weight_entangler = WeightEntangler() if self.entangle_op_weights else None

    def _set_pruner(self, config: dict) -> None:
        self.pruner: Pruner | None = None
        if config:
            self.pruner = Pruner(
                searchspace=self.search_space,
                prune_epochs=config.get("prune_epochs", []),
                prune_fractions=config.get("prune_fractions", []),
            )

    def _set_lora_toggler(self, lora_config: dict, lora_extra: dict) -> None:
        if lora_config.get("r", 0) == 0:
            self.lora_toggler = None
            return

        toggle_epochs = lora_extra.get("toggle_epochs")
        toggle_probability = lora_extra.get("toggle_probability")
        if toggle_epochs is not None:
            assert min(toggle_epochs) > lora_extra.get(
                "warm_epochs"
            ), "The first toggle epoch should be after the warmup epochs"
            self.lora_toggler = LoRAToggler(
                searchspace=self.search_space,
                toggle_epochs=toggle_epochs,
                toggle_probability=toggle_probability,
            )
        else:
            self.lora_toggler = None

    def _set_regularizer(self, config: dict) -> None:
        if config is None or len(config["active_reg_terms"]) == 0:
            self.regularizer = None
            return

        reg_terms: list[RegularizationTerm] = []
        for term in config["active_reg_terms"]:
            if term == "drnas":
                reg_terms.append(DrNASRegularizationTerm(**config["drnas_config"]))
            elif term == "flops":
                reg_terms.append(FLOPSRegularizationTerm(**config["flops_config"]))
            elif term == "fairdarts":
                reg_terms.append(
                    FairDARTSRegularizationTerm(**config["fairdarts_config"])
                )

        self.regularizer = Regularizer(
            reg_terms=reg_terms,
            reg_weights=config["reg_weights"],
            loss_weight=config["loss_weight"],
        )

    def _set_lambda_regularizer(self, config: dict) -> None:
        self.lambda_regularizer = (
            None if (config is None or len(config) == 0) else LambdaReg(**config)
        )

    def _set_profile(self, config: dict) -> None:
        assert self.sampler is not None

        self.search_space_handler = SearchSpaceHandler(
            sampler=self.sampler,
            edge_normalization=self.edge_normalization,
            partial_connector=self.partial_connector,
            perturbation=self.perturbator,
            dropout=self.dropout,
            weight_entangler=self.weight_entangler,
            lora_toggler=self.lora_toggler,
            lora_configs=config.get("lora"),
            pruner=self.pruner,
            is_arch_attention_enabled=config.get("is_arch_attention_enabled", False),
            regularizer=self.regularizer,
            lambda_regularizer=self.lambda_regularizer,
            use_auxiliary_skip_connection=config.get(
                "use_auxiliary_skip_connection", False
            ),
        )

    def _get_criterion(self, criterion_str: str) -> torch.nn.Module:
        criterion = CriterionType(criterion_str)
        if criterion == CriterionType.CROSS_ENTROPY:
            return torch.nn.CrossEntropyLoss()

        raise NotImplementedError

    def _get_optimizer(self, optim_str: str) -> Callable | None:
        optim = OptimizerType(optim_str)
        if optim == OptimizerType.ADAM:
            return torch.optim.Adam
        elif optim == OptimizerType.SGD:  # noqa: RET505
            return torch.optim.SGD
        if optim == OptimizerType.ASGD:
            return torch.optim.ASGD
        return None

    def _get_scheduler(
        self,
        scheduler_str: str,
        optimizer: torch.optim.Optimizer,
        num_epochs: int,
        eta_min: float = 0.0,
        config: dict | None = None,
    ) -> torch.optim.lr_scheduler.LRScheduler | None:
        if config is None:
            config = {}
        scheduler = SchedulerType(scheduler_str)
        if scheduler == SchedulerType.CosineAnnealingLR:
            return torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer=optimizer,
                T_max=num_epochs,
                eta_min=eta_min,
            )
        elif scheduler == SchedulerType.CosineAnnealingWarmRestart:  # noqa: RET505
            return torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
                optimizer=optimizer,
                T_0=config.get("T_0", 10),
                T_mult=config.get("T_mult", 1),
                eta_min=eta_min,
            )
        return None

    def _set_early_stopper(
        self, early_stopper: str | None, config: dict | None
    ) -> None:
        self.early_stopper: None | EarlyStopper = None

        if early_stopper is not None:
            assert config is not None, (
                "The configurations for the EarlyStopper is empty. "
                + "Use profile.configure_early_stopper() to fix it."
            )
            if early_stopper == "skip_connection":
                self.early_stopper = SkipConnectionEarlyStopper(**config)
            else:
                raise ValueError(f"Earlyt stopping method {early_stopper} not known!")

    def train_discrete_model(
        self,
        profile: DiscreteProfile,
        model_to_load: str | int | None = None,
        exp_runtime_to_load: str | None = None,
        use_supernet_checkpoint: bool = False,
    ) -> DiscreteTrainer:
        train_config = profile.get_trainer_config()
        searchspace_config = profile.get_searchspace_config(self.dataset.value)
        genotype_str = profile.get_genotype()
        run_name = profile.get_run_description()

        return self._train_discrete_model(
            searchspace_config=searchspace_config,
            train_config=train_config,
            model_to_load=model_to_load,
            exp_runtime_to_load=exp_runtime_to_load,
            use_supernet_checkpoint=use_supernet_checkpoint,
            genotype_str=genotype_str,
            run_name=run_name,
        )

    def get_discrete_model_from_genotype_str(
        self,
        search_space_str: str,
        genotype_str: str,
        searchspace_config: dict,
    ) -> torch.nn.Module:
        if search_space_str == SearchSpaceType.NB201.value:
            searchspace_config["genotype"] = NAS201Genotype.str2structure(genotype_str)
            discrete_model = NASBench201Model(**searchspace_config)
        elif search_space_str == SearchSpaceType.DARTS.value:
            searchspace_config["genotype"] = eval(genotype_str)
            if self.dataset in (
                DatasetType.CIFAR10,
                DatasetType.CIFAR100,
                DatasetType.AIRCRAFT,
            ):
                discrete_model = DARTSModel(**searchspace_config)
            elif self.dataset in (DatasetType.IMGNET16, DatasetType.IMGNET16_120):
                discrete_model = DARTSImageNetModel(**searchspace_config)
            else:
                raise ValueError("undefined discrete model for this dataset.")
        else:
            raise ValueError("undefined discrete model for this search space.")

        return discrete_model

    def get_discrete_model_from_supernet(
        self,
    ) -> SearchSpace:
        # A) Use the experiment's self.search_space of the experiment.
        if hasattr(self, "search_space"):
            if self.search_space.arch_parameters:
                model = self.search_space.discretize()
                return model  # type: ignore
            raise ValueError("need to be a supernet to be able to get the discrete net")
        raise Exception("Need a searchspace to be able to fetch a discrete model")

    # logger load_genotype function handles for both supernet and discrete model
    def get_genotype_str_from_checkpoint(
        self,
        model_to_load: str | int | None = None,
        use_supernet_checkpoint: bool = False,
    ) -> str:
        if model_to_load is not None:
            genotype_str = self.logger.load_genotype(
                model_to_load=model_to_load,
                use_supernet_checkpoint=use_supernet_checkpoint,
            )
            return genotype_str

        raise ValueError("is not a valid checkpoint.")

    def get_discrete_model(
        self,
        searchspace_config: dict,
        model_to_load: str | int | None = None,
        use_supernet_checkpoint: bool = False,
        use_expr_search_space: bool = False,
        genotype_str: str | None = None,
    ) -> tuple[torch.nn.Module, str]:
        # A) Use the experiment's self.search_space of the experiment.
        if use_expr_search_space:
            model = self.get_discrete_model_from_supernet()
            return model, model.get_genotype()  # type: ignore
        if model_to_load is not None:
            genotype_str = self.get_genotype_str_from_checkpoint(
                model_to_load=model_to_load,
                use_supernet_checkpoint=use_supernet_checkpoint,
            )
        elif sum(
            [
                (model_to_load is not None),
                use_supernet_checkpoint,
            ],
        ) == 0 and hasattr(self, "search_space"):
            genotype_str = self.search_space.get_genotype().tostr()  # type: ignore

        elif genotype_str is None:
            raise ValueError("genotype cannot be empty")

        model = self.get_discrete_model_from_genotype_str(
            self.searchspace_type.value,
            genotype_str,  # type: ignore
            searchspace_config,
        )
        return model, genotype_str  # type: ignore

    def _get_dataset(
        self,
        cutout: int,
        cutout_length: int,
        train_portion: float,
        *args: Any,  # noqa: ARG002
        **kwargs: Any,  # noqa: ARG002
    ) -> AbstractData:
        return get_dataset(
            dataset=self.dataset,
            domain=self.dataset_domain,
            root=self.dataset_dir,
            cutout=cutout,  # type: ignore
            cutout_length=cutout_length,  # type: ignore
            train_portion=train_portion,  # type: ignore
        )

    # refactor the name to train
    def _train_discrete_model(
        self,
        searchspace_config: dict,
        train_config: dict,
        model_to_load: str | int | None = None,
        exp_runtime_to_load: str | None = None,
        use_supernet_checkpoint: bool = False,
        use_expr_search_space: bool = False,
        genotype_str: str | None = None,
        run_name: str = "discrete_run",
    ) -> DiscreteTrainer:
        # should not care where the model comes from => genotype should be a
        # different function
        self._set_seed(self.seed)

        load_last_run = model_to_load is not None and not exp_runtime_to_load
        self.logger = Logger(
            log_dir="logs",
            exp_name=self.exp_name,
            search_space=self.searchspace_type.value,
            dataset=str(self.dataset.value),
            seed=self.seed,
            runtime=exp_runtime_to_load,
            use_supernet_checkpoint=use_supernet_checkpoint,
            last_run=load_last_run,
        )

        # different options to train a discrete model:
        # A) Use the experiment's self.search_space of the experiment.
        # B) From a supernet checkpoint, load, and discretize to get the model.
        # C) From a discerete checkpoint, load the model.
        # D) pass a genotype from the prompt to build a discrete model.
        # E) just use the default genotype which is set in the discrete_profile.

        model, genotype_str = self.get_discrete_model(
            searchspace_config=searchspace_config,
            model_to_load=model_to_load,
            use_supernet_checkpoint=use_supernet_checkpoint,
            use_expr_search_space=use_expr_search_space,
            genotype_str=genotype_str,
        )
        model.to(device=DEVICE)

        n_params_model = sum(p.numel() for p in model.parameters())
        train_config["n_params_model"] = n_params_model

        # TODO: do i need this line?
        if use_supernet_checkpoint:
            model_to_load = None
            use_supernet_checkpoint = False
            self.logger.use_supernet_checkpoint = False
            self.logger.set_up_new_run()

        self.logger.save_genotype(genotype_str)

        if train_config.get("use_ddp", False) is True:
            assert torch.distributed.is_initialized(), "DDP is not initialized!"
            world_size = dist_utils.get_world_size()
            train_config["lr"] *= world_size  # type: ignore
            train_config["learning_rate_min"] *= world_size  # type: ignore

        Arguments = namedtuple(  # type: ignore
            "Configure", " ".join(train_config.keys())  # type: ignore
        )
        trainer_arguments = Arguments(**train_config)  # type: ignore

        data = self._get_dataset(
            cutout=trainer_arguments.cutout,  # type: ignore
            cutout_length=trainer_arguments.cutout_length,  # type: ignore
            train_portion=trainer_arguments.train_portion,  # type: ignore
        )

        w_optimizer = self._get_optimizer(trainer_arguments.optim)(  # type: ignore
            model.parameters(),
            trainer_arguments.lr,  # type: ignore
            **trainer_arguments.optim_config,  # type: ignore
        )

        w_scheduler = self._get_scheduler(
            scheduler_str=trainer_arguments.scheduler,  # type: ignore
            optimizer=w_optimizer,
            num_epochs=trainer_arguments.epochs,  # type: ignore
            eta_min=trainer_arguments.learning_rate_min,  # type: ignore
            config=train_config.get("scheduler_config", {}),  # type: ignore
        )

        criterion = self._get_criterion(
            criterion_str=trainer_arguments.criterion  # type: ignore
        )

        trainer = DiscreteTrainer(
            model=model,
            data=data,
            model_optimizer=w_optimizer,
            scheduler=w_scheduler,
            criterion=criterion,
            logger=self.logger,
            batch_size=trainer_arguments.batch_size,  # type: ignore
            use_ddp=trainer_arguments.use_ddp,  # type: ignore
            print_freq=trainer_arguments.print_freq,  # type: ignore
            drop_path_prob=trainer_arguments.drop_path_prob,  # type: ignore
            aux_weight=trainer_arguments.auxiliary_weight,  # type: ignore
            model_to_load=model_to_load,
            checkpointing_freq=trainer_arguments.checkpointing_freq,  # type: ignore
            epochs=trainer_arguments.epochs,  # type: ignore
            debug_mode=self.debug_mode,
        )
        if self.log_with_wandb:
            self._init_wandb(run_name, config=train_config)

        trainer.train(
            epochs=trainer_arguments.epochs,  # type: ignore
            log_with_wandb=self.log_with_wandb,
        )

        trainer.test(log_with_wandb=self.log_with_wandb)

        return trainer

    def _initialize_configurable_trainer(
        self,
        config: dict,
        model_to_load: str | int | None = None,
    ) -> ConfigurableTrainer:
        Arguments = namedtuple(  # type: ignore
            "Configure", " ".join(config["trainer"].keys())  # type: ignore
        )
        trainer_arguments = Arguments(**config["trainer"])  # type: ignore

        criterion = self._get_criterion(
            criterion_str=trainer_arguments.criterion  # type: ignore
        )

        data = self._get_dataset(
            cutout=trainer_arguments.cutout,  # type: ignore
            cutout_length=trainer_arguments.cutout_length,  # type: ignore
            train_portion=trainer_arguments.train_portion,  # type: ignore
        )

        model = self.search_space

        w_optimizer = self._get_optimizer(trainer_arguments.optim)(  # type: ignore
            model.model_weight_parameters(),
            trainer_arguments.lr,  # type: ignore
            **config["trainer"].get("optim_config", {}),  # type: ignore
        )

        w_scheduler = self._get_scheduler(
            scheduler_str=trainer_arguments.scheduler,  # type: ignore
            optimizer=w_optimizer,
            num_epochs=trainer_arguments.epochs,  # type: ignore
            eta_min=trainer_arguments.learning_rate_min,  # type: ignore
            config=config["trainer"].get("scheduler_config", {}),  # type: ignore
        )

        if self.edge_normalization and hasattr(model, "beta_parameters"):
            arch_optimizer = self._get_optimizer(
                trainer_arguments.arch_optim  # type: ignore
            )(
                [*model.arch_parameters, *model.beta_parameters],
                lr=config["trainer"].get("arch_lr", 0.001),  # type: ignore
                **config["trainer"].get("arch_optim_config", {}),  # type: ignore
            )
        else:
            arch_optimizer = self._get_optimizer(
                trainer_arguments.arch_optim  # type: ignore
            )(
                model.arch_parameters,
                lr=config["trainer"].get("arch_lr", 0.001),  # type: ignore
                **config["trainer"].get("arch_optim_config", {}),  # type: ignore
            )

        trainer = ConfigurableTrainer(
            model=model,
            data=data,
            model_optimizer=w_optimizer,
            arch_optimizer=arch_optimizer,
            scheduler=w_scheduler,
            criterion=criterion,
            logger=self.logger,
            batch_size=trainer_arguments.batch_size,  # type: ignore
            use_data_parallel=trainer_arguments.use_data_parallel,  # type: ignore
            model_to_load=model_to_load,
            checkpointing_freq=trainer_arguments.checkpointing_freq,  # type: ignore
            epochs=trainer_arguments.epochs,  # type: ignore
            debug_mode=self.debug_mode,
            query_dataset=self.dataset.value,
            benchmark_api=self.benchmark_api,
            early_stopper=self.early_stopper,
        )

        return trainer

    def select_perturbation_based_arch(
        self,
        profile: BaseProfile,
        model_source: Literal["supernet", "arch_selection"] = "supernet",
        model_to_load: str | int | None = None,
        exp_runtime_to_load: str | None = None,
        log_with_wandb: bool = False,
        run_name: str = "darts-pt",
        src_folder_path: str | None = None,
    ) -> PerturbationArchSelection:
        # find pt_configs in the profile
        if model_to_load is not None:
            # self.searchspace is not trained
            last_run = False
            if not exp_runtime_to_load:
                last_run = True

            if model_source == "supernet":
                arch_selection = False
            elif model_source == "arch_selection":
                assert (
                    model_to_load != "best"
                ), "Cannot load best model for arch selection"
                arch_selection = True
            else:
                raise AttributeError("Illegal model source provided")

            self.logger = Logger(
                log_dir="logs",
                exp_name=self.exp_name,
                dataset=str(self.dataset.value),
                search_space=self.searchspace_type.value,
                seed=self.seed,
                runtime=exp_runtime_to_load,
                use_supernet_checkpoint=True,
                arch_selection=arch_selection,
                last_run=last_run,
                custom_log_path=src_folder_path,
            )
        else:
            assert (
                model_source == "supernet"
            ), "Model source can be arch_selection only with loading parameters"
            # use the self.logger
            # that is already in the experiment from last supernet training

        # initialize searchspace handler
        config = profile.get_config()

        self.sampler_str = SamplerType(profile.sampler_type)
        self.perturbator_str = PerturbatorType(profile.perturb_type)
        self.is_partial_connection = profile.is_partial_connection
        self.dropout_p = profile.dropout
        self.edge_normalization = profile.is_partial_connection
        self.entangle_op_weights = profile.entangle_op_weights

        self._init_components(
            self.searchspace_type,
            self.sampler_str,
            self.perturbator_str,
            config=config,
        )

        # Load model from trainer's init
        trainer = self._initialize_configurable_trainer(
            config=config,
            model_to_load=model_to_load,
        )
        search_space_handler = self.search_space_handler
        search_space_handler.adapt_search_space(trainer.model)

        # Load from supernet
        if model_source == "supernet":
            trainer._init_experiment_state(
                search_space_handler=search_space_handler,
                setup_new_run=False,
                warm_epochs=config["trainer"].get(  # type: ignore
                    "lora_warm_epochs", 0
                ),
            )

            # reroute logger
            self.logger = Logger(
                log_dir="logs",
                exp_name=self.exp_name,
                dataset=str(self.dataset.value),
                search_space=self.searchspace_type.value,
                seed=self.seed,
                use_supernet_checkpoint=True,
                arch_selection=True,
            )
            trainer.model_to_load = None
            # TODO: do we need to have the next line?
            trainer.start_epoch = 0
            trainer.logger = self.logger

        trainer._init_experiment_state()

        if log_with_wandb:
            self._init_wandb(run_name, config)

        arch_selector = PerturbationArchSelection(
            trainer,
            config["pt_selection"].get("projection_criteria", "acc"),
            config["pt_selection"].get("projection_interval", 10),
            log_with_wandb=log_with_wandb,
        )
        arch_selector.select_architecture()

        return arch_selector


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        "Fine tuning and training searched architectures", add_help=False
    )
    parser.add_argument(
        "--searchspace",
        default="nb201",
        help="search space in (darts, nb201, nb1shot1, tnb101)",
        type=str,
    )
    parser.add_argument(
        "--sampler",
        default="gdas",
        help="samplers in (darts, drnas, gdas, snas)",
        type=str,
    )
    parser.add_argument(
        "--perturbator",
        default="none",
        help="Type of perturbation in (none, random, adverserial)",
        type=str,
    )
    parser.add_argument(
        "--is_partial_connector",
        action="store_true",
        default=False,
        help="Enable/Disable partial connection",
    )
    parser.add_argument(
        "--dropout",
        default=None,
        help="Dropout probability. 0 <= p < 1.",
        type=float,
    )
    parser.add_argument("--dataset", default="cifar10", type=str)
    parser.add_argument("--logdir", default="./logs", type=str)
    parser.add_argument("--seed", default=444, type=int)
    parser.add_argument("--epochs", default=3, type=int)
    parser.add_argument("--exp_name", default="test", type=str)
    parser.add_argument(
        "--model_to_load",
        type=validate_model_to_load_value,
        help="if str, could be best, last. If int, load the checkpoint of that epoch",
        default=None,
    )
    parser.add_argument(
        "--use_supernet_checkpoint",
        action="store_true",
        default=False,
        help="If you would like to load from last, best, or a specific epoch",
    )
    parser.add_argument(
        "--runtime",
        # default="",
        help="if you want to start from in a certain runtime",
        type=str,
    )

    parser.add_argument(
        "--oles",
        action="store_true",
        default=False,
        help="freezes weights if it passes the threshold",
    )

    parser.add_argument(
        "--calc_gm_score",
        action="store_true",
        default=False,
        help="calculates gm scores during training the supernet",
    )

    args = parser.parse_args()
    IS_DEBUG_MODE = True
    log_with_wandb = IS_DEBUG_MODE is False

    searchspace = SearchSpaceType(args.searchspace)
    dataset = DatasetType(args.dataset)
    args.epochs = 3

    profile = GDASProfile(
        searchspace=searchspace.value,
        epochs=args.epochs,
        is_partial_connection=args.is_partial_connector,
        perturbation=args.perturbator,
        dropout=args.dropout,
        oles=args.oles,
        calc_gm_score=args.calc_gm_score,
    )

    config = profile.get_config()

    if args.runtime:
        assert check_date_format(args.runtime)

    experiment = Experiment(
        search_space=searchspace,
        dataset=dataset,
        seed=args.seed,
        log_with_wandb=log_with_wandb,
        debug_mode=IS_DEBUG_MODE,
        exp_name=args.exp_name,
    )

    # trainer = experiment.run_with_profile(
    #     profile,
    #     model_to_load=args.model_to_load,
    # )

    discrete_profile = DiscreteProfile(searchspace.value)

    exp_runtime_to_load = args.runtime if args.runtime != "" else None
    discret_trainer = experiment.train_discrete_model(
        discrete_profile,
        model_to_load=args.model_to_load,
        use_supernet_checkpoint=args.use_supernet_checkpoint,
        exp_runtime_to_load=exp_runtime_to_load,
    )

    if log_with_wandb:
        wandb.finish()  # type: ignore
